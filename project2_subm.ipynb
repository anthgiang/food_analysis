{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.spatial.distance import cdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import pdist, squareform\n",
    "def VAT(R):\n",
    "    R = np.array(R)\n",
    "    N, M = R.shape\n",
    "    if N != M:\n",
    "        R = squareform(pdist(R))\n",
    "    J = list(range(0, N))\n",
    "    \n",
    "    y = np.max(R, axis=0)\n",
    "    i = np.argmax(R, axis=0)\n",
    "    j = np.argmax(y)\n",
    "    y = np.max(y)\n",
    "    \n",
    "    I = i[j]\n",
    "    del J[I]\n",
    "    \n",
    "    y = np.min(R[I,J], axis=0)\n",
    "    j = np.argmin(R[I,J], axis=0)\n",
    "    I = [I, J[j]]\n",
    "    J = [e for e in J if e != J[j]]\n",
    "    C = [1,1]\n",
    "\n",
    "    for r in range(2, N-1):\n",
    "        y = np.min(R[I,:][:,J], axis=0)\n",
    "        i = np.argmin(R[I,:][:,J], axis=0)\n",
    "        j = np.argmin(y)\n",
    "        y = np.min(y)\n",
    "        I.extend([J[j]])\n",
    "        J = [e for e in J if e != J[j]]\n",
    "        C.extend([i[j]])\n",
    "\n",
    "    y = np.min(R[I,:][:,J], axis=0)\n",
    "    i = np.argmin(R[I,:][:,J], axis=0)\n",
    "\n",
    "    I.extend(J)\n",
    "    C.extend(i)\n",
    "\n",
    "    RI = list(range(N))\n",
    "    for idx, val in enumerate(I):\n",
    "        RI[val] = idx\n",
    "\n",
    "    RV = R[I,:][:,I]\n",
    "    return RV.tolist(), C, I\n",
    "\n",
    "\n",
    "def entropy(probs):\n",
    "    \n",
    "    return -probs.dot(np.log2(probs))\n",
    "\n",
    "\n",
    "def mutual_info(df):\n",
    "    \n",
    "    Hx = entropy(df.iloc[:,0].value_counts(normalize=True, sort=False))\n",
    "    Hy = entropy(df.iloc[:,1].value_counts(normalize=True, sort=False))\n",
    "    \n",
    "    counts = df.groupby(list(df.columns.values)).size()\n",
    "    probs = counts/ counts.values.sum()\n",
    "    H_xy = entropy(probs)\n",
    "\n",
    "    # Mutual Information\n",
    "    I_xy = Hx + Hy - H_xy\n",
    "    MI = I_xy\n",
    "    NMI = I_xy/min(Hx,Hy) #I_xy/np.sqrt(H_x*H_y)\n",
    "    return MI\n",
    "    return {'H_'+list(df)[0]:Hx,'H_'+list(df)[1]:Hy,'MI':MI,'NMI':NMI} \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food = pd.read_csv(\"food_nutrient_2011_13_AHS.csv\", header=0,low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QUESTION 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Question 1\n",
    "\n",
    "\n",
    "###1a\n",
    "\n",
    "foodcut = food.loc[:, \"Energy, with dietary fibre (kJ)\":\"Total trans fatty acids (mg)\"]\n",
    "foodcut.columns = food.loc[:, \"Energy, with dietary fibre (kJ)\":\"Total trans fatty acids (mg)\"].columns\n",
    "\n",
    "###1b\n",
    "\n",
    "foodcut = foodcut.astype(float)\n",
    "scaler = StandardScaler().fit(foodcut)\n",
    "foodscaled = scaler.transform(foodcut)\n",
    "\n",
    "###1c\n",
    "\n",
    "print(\"***\")\n",
    "print(\"Number of rows: \", foodscaled.shape[0])\n",
    "print(\"Number of columns: \", foodscaled.shape[1])\n",
    "print(\"Min: \", round(foodscaled.min(), 1))\n",
    "print(\"Max: \", round(foodscaled.max(), 1))\n",
    "print(\"Mean: \", round(foodscaled.mean(), 1))\n",
    "print(\"Standard Deviation: \", round(foodscaled.std(), 1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QUESTION 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 2\n",
    "\n",
    "\n",
    "###2a\n",
    "\n",
    "EnergyLevel = [1 if x is x>1000 else 0 for x in food[\"Energy, with dietary fibre (kJ)\"]]\n",
    "\n",
    "###2b\n",
    "\n",
    "sklearn_pca = PCA(n_components=2)   #we want just the first two PCs\n",
    "foodreduced = sklearn_pca.fit_transform(foodscaled)\n",
    "\n",
    "###2c\n",
    "\n",
    "color = [\"red\" if x is x==1 else \"blue\" for x in EnergyLevel]\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [15, 10]    # change plot size\n",
    "plt.scatter(foodreduced[:,0],foodreduced[:,1],  marker=\"x\", c=color)   #plot the PC's\n",
    "plt.scatter([],[], marker=\"x\",color='r')\n",
    "plt.xlabel('1st Principal Component', fontsize=15)\n",
    "plt.ylabel('2nd Principal Component', fontsize=15)\n",
    "plt.legend([\"Low Energy\", \"High Energy\"])\n",
    "plt.title(\"PCA on Food (2 Components)\", fontsize=20)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2d\n",
    "\n",
    "The scatter plot shows the standardised food data reduced down to the 2 dimensions with the most variation. The types of food with low energy are clustered together, similiar to the high energy food. Principal Component 1 is most likely to be the overall amount of energy in a type of food as mostly all the high energy (red marks) foods are to the right of the low energy (blue marks) food. Principal component 2 is most likely all the vitamins and nutrients as it has a high variability and generally lower energy food has more of it. The advantage of using PCA on this dataset is that it is much easier to visualise due to the small number of features. The disadvantage is that most of the variability is lost as only about 0.25 of the variance is captured with a PCA of 2 components on this dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QUESTION 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 3\n",
    "\n",
    "\n",
    "###3a\n",
    "\n",
    "food[\"Survey ID\"] = food[\"Survey ID\"].astype(\"str\")\n",
    "category = [x[:2] for x in food[\"Survey ID\"]]    # slice the first two characters\n",
    "food[\"Food Category\"] = category\n",
    "\n",
    "###3b\n",
    "\n",
    "foodscaledsample = pd.DataFrame(foodscaled)    # convert into a dataframe\n",
    "foodscaledsample[\"Food Category\"] = category    # add the Food Category column\n",
    "foodscaledsample = foodscaledsample.loc[foodscaledsample[\"Food Category\"].isin([\"13\",\"20\",\"24\"])]    # select foods\n",
    "foodscaledsample = foodscaledsample.drop(columns=[\"Food Category\"])\n",
    "\n",
    "###3c\n",
    "\n",
    "RV, C, I = VAT(foodscaledsample)\n",
    "\n",
    "###3d\n",
    "\n",
    "plt.axis('equal')\n",
    "x=sns.heatmap(RV,cmap='Spectral',xticklabels=False,yticklabels=False)\n",
    "x.set(xlabel=\"Features\", ylabel=\"Features\")\n",
    "plt.title(\"Heat Map of Dissimilarity Matrix\", fontsize=15)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3e\n",
    "\n",
    "2 clusters can be identified from the heatmap. This is not expected as 3 food categories were selected from the foodscaled data and so 3 clusters were expected. The use of different colormaps produce visualisations of varying usefulness as some colormaps make it hard to distinguish the data because of the colour. A Colormap that is sequential is good for this dataset as the data has ordering. The colormap also should change colour over a small change in data value as this data set has is very evenly distributed and small differences need to be identified.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QUESTION 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 4\n",
    "\n",
    "###4a \n",
    "\n",
    "foodscaled = pd.DataFrame(foodscaled)\n",
    "foodscaled.columns = foodcut.columns\n",
    "\n",
    "sse = {}\n",
    "for k in range(2, 25):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=100).fit(foodscaled)\n",
    "    sse[k] = kmeans.inertia_ # Inertia: Sum of distances of samples to their closest cluster center\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0)\n",
    "plt.figure()\n",
    "plt.plot(list(sse.keys()), list(sse.values()), \"r-x\")\n",
    "plt.xlabel(\"Number of clusters\", fontsize=15)\n",
    "plt.ylabel(\"SSE\", fontsize=15)\n",
    "plt.title(\"Number of Clusters vs SSE\", fontsize=20)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4b\n",
    "\n",
    "As the number of clusters increases, the SSE decreases monotonically. The shape and trend of the graph looks linear and is smooth with an elbow being very hard to spot. It is not obvious where the elbow is but if one must be chosen, it would be at around 16 clusters. This is to be expected as from previous visualisations such as the heatmap, it is evident that the data is very evenly distributed and obvious clusters would be hard to identify."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QUESTION 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Question 5\n",
    "\n",
    "\n",
    "###5a\n",
    "\n",
    "foodcorr = food.loc[:, \"Energy, with dietary fibre (kJ)\":\"Added sugars (g)\"].corr(method='pearson')\n",
    "x=sns.heatmap(foodcorr,cmap='RdYlBu',xticklabels=True,yticklabels=True)\n",
    "plt.title(\"Pearson correlation matrix for the first 10 nutrients\", fontsize=20)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###5b code\n",
    "\n",
    "mi = {}\n",
    "\n",
    "x = pd.cut(food[\"Protein (g)\"], 2, labels=False)\n",
    "y = pd.cut(food[\"Energy, with dietary fibre (kJ)\"], 2, labels=False)\n",
    "temp = pd.DataFrame({'0': x,'1': y})\n",
    "mi[2] = mutual_info(temp)\n",
    "    \n",
    "for i in range(10,200,10):\n",
    "    x = pd.cut(food[\"Protein (g)\"], i, labels=False)\n",
    "    y = pd.cut(food[\"Energy, with dietary fibre (kJ)\"], i, labels=False)\n",
    "    temp = pd.DataFrame({'0': x,'1': y})\n",
    "    mi[i] = mutual_info(temp)\n",
    "\n",
    "plt.plot(list(mi.keys()), list(mi.values()), \"r-x\")\n",
    "plt.xlabel(\"Number of equal-width bins\", fontsize=15)\n",
    "plt.ylabel(\"Mutual Information\", fontsize=15)\n",
    "plt.title(\"Number of equal-width bins vs Mutual Information, for Protein (g) and Energy, with dietary fibre (kJ)\", fontsize=20)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5b\n",
    "\n",
    "As the number of equal-width bins increases, the mutual information for protein and energy increases greatly from 2 to 30 bins and then steadily from 30 bins onwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###5c code\n",
    "\n",
    "def get_redundant_pairs(df):\n",
    "    # Get the diagonal and lower triangular pairs of the correlation matrix\n",
    "    pairs_to_drop = set()\n",
    "    cols = df.columns\n",
    "    for i in range(0, df.shape[1]):\n",
    "        for j in range(0, i+1):\n",
    "            pairs_to_drop.add((cols[i], cols[j]))\n",
    "    return pairs_to_drop\n",
    "\n",
    "def get_top_abs_correlations_pearson(df, n):\n",
    "    au_corr = df.corr().unstack()    # remove unwanted mutual information of certain pairs\n",
    "    labels_to_drop = get_redundant_pairs(df)\n",
    "    # remove unwanted mutual information of certain pairs and sort by highest to lowest\n",
    "    au_corr = au_corr.drop(labels=labels_to_drop).sort_values(ascending=False)\n",
    "    return au_corr[0:n]    # return the top n\n",
    "\n",
    "    \n",
    "\n",
    "def get_top_abs_correlations_mi(df, n):\n",
    "    # calculate MI dictionary\n",
    "    mi_corr_dict = {}\n",
    "    for i in range(df.shape[1]):\n",
    "        mi_corr_dict[df.columns[i]] = []\n",
    "        for j in range(df.shape[1]):\n",
    "            x = pd.cut(df[df.columns[i]], 20, labels=False)\n",
    "            y = pd.cut(df[df.columns[j]], 20, labels=False)\n",
    "            temp = pd.DataFrame({'0': x,'1': y})\n",
    "            mi_corr_dict[df.columns[i]].append(mutual_info(temp))\n",
    "    # turn the MI dictionary into a matrix\n",
    "    mi_corr_df = pd.DataFrame(mi_corr_dict)\n",
    "    \n",
    "    # rename all the rows to be the same as the columns\n",
    "    cols = mi_corr_df.columns.tolist()\n",
    "    mapper = {}\n",
    "    j = 0\n",
    "    for i in cols:\n",
    "        mapper[j] = i\n",
    "        j += 1\n",
    "    mi_corr_df = mi_corr_df.rename(mapper, axis=\"index\")\n",
    "    \n",
    "    au_corr = mi_corr_df.unstack()    # turn the dataframe into a series\n",
    "    labels_to_drop = get_redundant_pairs(df)\n",
    "    # remove unwanted mutual information of certain pairs and sort by highest to lowest\n",
    "    au_corr = au_corr.drop(labels=labels_to_drop).sort_values(ascending=False)\n",
    "    return au_corr[0:n]    # return the top n\n",
    "\n",
    "print(\"Top 10 Pearson feature pairs\")\n",
    "print(get_top_abs_correlations_pearson(foodcut, 10))\n",
    "print(\"***\")\n",
    "print(\"Top 10 Mutual Information feature pairs\")\n",
    "print(get_top_abs_correlations_mi(foodcut, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5c\n",
    "\n",
    "The top Pearson pairs and the top Mutual Information pairs do share some feature pairs such as, Energy with dietary fibre & Energy without dietary fibre. The difference between the top Pearson and the top Mutual Information is that the Pearson can only find linear relationships between features while Mutual Information can find non-linear relationships as well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QUESTION 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Question 6\n",
    "\n",
    "\n",
    "###6a\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(foodscaled, category, train_size=0.80, test_size=0.20)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "print(\"***\")\n",
    "print(\"Q6.a: Train Test Split Results\")\n",
    "print(\"X_train matrix: \", X_train.shape)\n",
    "print(\"y_train labels: \", len(y_train))\n",
    "print(\"X_test matrix: \", X_test.shape)\n",
    "print(\"y_test labels: \", len(y_test))\n",
    "print(\"***\")\n",
    "\n",
    "###6b\n",
    "\n",
    "maxdepth = {}\n",
    "for k in range(1,41):\n",
    "    dt = DecisionTreeClassifier(criterion=\"entropy\", max_depth=k)\n",
    "    dt.fit(X_train, y_train)\n",
    "    y_pred=dt.predict(X_test)\n",
    "    maxdepth[k] = accuracy_score(y_test, y_pred)\n",
    "\n",
    "plt.plot(list(maxdepth.keys()), list(maxdepth.values()), \"r-x\")\n",
    "plt.xlabel(\"Depth of Decision Tree\", fontsize=15)\n",
    "plt.ylabel(\"Accuracy\", fontsize=15)\n",
    "plt.title(\"Depth of Decision Tree vs Accuracy\", fontsize=20)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6c\n",
    "\n",
    "The accuracy of the decision tree increases dramatically from depth 1 to depth 8 and starts to peak at depth 10. After a depth of 10, the accuracy does not increase anymore. A reason for the rapid increase from depth 1 to 8 is because the decision tree's depth is limited to a small number and not all the data points can be classified by just a few features. The accuracy then plateaus after a certain depth (in this case depth 10) as no more additonal information can be gained about the data point. The decision tree is also very accurate as the training data is normalised with the testing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QUESTION 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 7\n",
    "\n",
    " ###7a\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "y_pred_test = knn.predict(X_test)\n",
    "y_pred_train = knn.predict(X_train)\n",
    "\n",
    "test_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "train_accuracy = accuracy_score(y_train, y_pred_train)\n",
    "\n",
    "print(\"***\")\n",
    "print(\"Q7a: Food category prediction using k-NN (k=1)\")\n",
    "print(\"Train accuracy: \", round(train_accuracy*100, 1),\"%\")\n",
    "print(\"Test accuracy: \", round(test_accuracy*100, 1),\"%\")\n",
    "print(\"***\")\n",
    "\n",
    "##7b\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred_test = knn.predict(X_test)\n",
    "y_pred_train = knn.predict(X_train)\n",
    "\n",
    "test_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "train_accuracy = accuracy_score(y_train, y_pred_train)\n",
    "\n",
    "print(\"***\")\n",
    "print(\"Q7b: Food category prediction using k-NN (k=3)\")\n",
    "print(\"Train accuracy: \", round(train_accuracy*100, 1),\"%\")\n",
    "print(\"Test accuracy: \", round(test_accuracy*100, 1),\"%\")\n",
    "print(\"***\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7c\n",
    "\n",
    "Testing the training data with k=1 will always yield 100% accuracy but with k=3 it will not be 100% accurate as there is added noise in the prediction. The test data accuracy for k=1 is generally higher than when k=3 also possibly because of the added noise from the extra comparisons with the other 2 data points.\n",
    "\n",
    "## Question 7d\n",
    "\n",
    "The accuracy of the KNN predictions are over optimistic because the testing data is being standardised with the training data with a mean of 0 and std of 1. The predictions are therefore more accurate because testing data is not independent of the training data. To fix this, the scaler (normalisation) should be applied to the training data only. Then the same scalar is applied onto the testing data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 8 code\n",
    "\n",
    "# feature generation\n",
    "foodscaled[\"Total fat (g) x Protein (g)\"] = foodscaled[\"Total fat (g)\"]*foodscaled[\"Protein (g)\"]\n",
    "\n",
    "# clustering labels\n",
    "kmeans = KMeans(n_clusters=16, random_state=100).fit(foodscaled)\n",
    "foodscaled[\"clusterlabel\"] = kmeans.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 8 discussion\n",
    "\n",
    "feature generation is unlikely to increase accuracy as it is derived data from the data already there. It only adds noise to the data and reduces accuracy. Clustering labels can increase accuracy as it removes the features that do not have any relation to the label and thus keep all the \"important\" features."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
